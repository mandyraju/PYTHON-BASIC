{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "Answer:\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. Its primary goal is to find the best decision boundary—called a hyperplane—that separates classes of data with the maximum margin (greatest possible distance from the nearest data points of each class, known as support vectors).​\n",
        "\n",
        "How does it work?\n",
        "\n",
        "Given labeled training data, SVM searches for the hyperplane that divides the classes most clearly, maximizing the margin between them.\n",
        "\n",
        "If data is linearly separable, SVM draws a straight line (or its higher-dimensional equivalent) to separate the two classes. The margin is maximized to help the model generalize to new, unseen data.\n",
        "\n",
        "If data is not linearly separable, SVM can use a function (called a kernel) to map data to a higher-dimensional space where a hyperplane can separate the classes\n",
        "\n",
        "Question 2: Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "Answer:\n",
        "Assumes that data is perfectly separable—no misclassifications allowed.\n",
        "\n",
        "The hyperplane is chosen so all data points are on the correct side, and the margin is maximized.​\n",
        "\n",
        "Limitation: Fails if the dataset has overlap, noise, or outliers; not practical for real-world data, which is usually imperfect.\n",
        "\n",
        "Soft Margin SVM:\n",
        "\n",
        "Allows some points to be on the wrong side of the margin. This is controlled with slack variables and a penalty term, balancing margin width and misclassification cost.\n",
        "\n",
        "The model can tolerate misclassified examples or margin violations and is thus more flexible for real-life datasets.\n",
        "\n",
        "Parameter C: Controls the tradeoff. High C puts more penalty on misclassification; low C allows a wider margin with more possible misclassification\n",
        "\n",
        "Question 3: What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "\n",
        "Answer:\n",
        "The Kernel Trick is a technique that allows SVMs to solve non-linear classification problems. It works by implicitly mapping data into a higher-dimensional space (without explicitly performing the transformation) so that a linear hyperplane can be used to separate data that isn't linearly separable in the original space.​\n",
        "\n",
        "How does it work?\n",
        "\n",
        "Instead of calculating new features for every data point, a kernel function calculates similarities between points in the higher-dimensional space.\n",
        "\n",
        "SVM uses these kernel values to build an optimal hyperplane in that space.\n",
        "\n",
        "Question 4: What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "Answer:\n",
        "A Naïve Bayes Classifier is a probabilistic machine learning algorithm based on Bayes’ Theorem, used for classifying data into distinct categories.\n",
        "\n",
        "Key idea:\n",
        "\n",
        "It calculates the probability that a data point belongs to a certain class, given the input features.\n",
        "\n",
        "Assigns the class label with the highest probability to each data point.\n",
        "\n",
        "Why is it “naïve”?\n",
        "\n",
        "The term naïve comes from its “naïve” assumption that all features are independent of each other—i.e., the value of one feature does not affect the value of another. In most real-world data, this is rarely true, but the classifier still often works surprisingly well.\n",
        "\n",
        "Question 5: Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
        "\n",
        "Answer:\n",
        "Gaussian Naïve Bayes\n",
        "\n",
        "Assumes features follow a normal (Gaussian) distribution.\n",
        "\n",
        "Best suited for continuous numeric data (e.g., age, income).\n",
        "\n",
        "Used for: Iris dataset classification, medical data, sensor measurements.\n",
        "\n",
        "Multinomial Naïve Bayes\n",
        "\n",
        "Assumes features are counts or frequencies of events.\n",
        "\n",
        "Best for discrete data: word counts in documents, bag-of-words model for text classification.\n",
        "\n",
        "Used for: Email spam detection (using word count/features), document classification.\n",
        "\n",
        "Bernoulli Naïve Bayes\n",
        "\n",
        "Assumes features are binary (only two outcomes: 0 or 1).\n",
        "\n",
        "Each feature represents whether a token/word is present or not.\n",
        "\n",
        "Used for: Text classification with binary term occurrence (e.g., is the word 'offer' present?), suitable for short texts or feature sets with on/off indicators.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ngD4K2wWZhkT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpwGy60uXOuD",
        "outputId": "c7292df0-cfad-4a82-d7d9-ad601a41fa9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9933333333333333\n",
            "Support Vectors:\n",
            " [[5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [5.9 3.  5.1 1.8]]\n"
          ]
        }
      ],
      "source": [
        "#Question 6: Write a Python program to:\n",
        "#● Load the Iris dataset\n",
        "#● Train an SVM Classifier with a linear kernel\n",
        "#● Print the model's accuracy and support vectors.\n",
        "#Answer\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "svm = SVC(kernel=\"linear\")\n",
        "svm.fit(X, y)\n",
        "\n",
        "print(\"Accuracy:\", svm.score(X, y))\n",
        "print(\"Support Vectors:\\n\", svm.support_vectors_)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to:\n",
        "#● Load the Breast Cancer dataset\n",
        "#● Train a Gaussian Naïve Bayes model\n",
        "#● Print its classification report including precision, recall, and F1-score.\n",
        "#Answer\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "cancer = load_breast_cancer()\n",
        "X, y = cancer.data, cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDc-Laf3cZN2",
        "outputId": "34685a4d-c7c8-4ac2-dbab-e50725b06e1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96        43\n",
            "           1       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 8: Write a Python program to:\n",
        "#● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "#C and gamma.\n",
        "#● Print the best hyperparameters and accuracy.\n",
        "\n",
        "#Answer:\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# GridSearchCV\n",
        "param_grid = {'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]}\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print('Best Params:', grid.best_params_)\n",
        "print('Accuracy:', grid.score(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_ZmZsNBcuE8",
        "outputId": "455f0961-858c-4bd1-9d74-acdeaccfe9b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Params: {'C': 10, 'gamma': 0.01}\n",
            "Accuracy: 0.6666666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 9: Write a Python program to:\n",
        "#● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "#sklearn.datasets.fetch_20newsgroups).\n",
        "#● Print the model's ROC-AUC score for its predictions.\n",
        "#Answer:\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "data = fetch_20newsgroups(subset='all', categories=['sci.space', 'rec.autos'])\n",
        "X, y = data.data, data.target\n",
        "\n",
        "vectorizer = CountVectorizer(binary=False)\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# ROC-AUC\n",
        "y_prob = nb.predict_proba(X_test)[:,1]\n",
        "print('ROC-AUC:', roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zGzJzSwdD0d",
        "outputId": "8fd8b91f-8dce-4989-e22a-aaf99cd8787c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC: 0.9971941638608306\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution\n",
        "\n",
        "Answer:\n",
        "Preprocessing: Vectorize text using methods like TF-IDF or CountVectorizer. Handle missing values by imputing with empty strings or dropping incomplete samples if needed.\n",
        "\n",
        "Model Choice: Start with Naïve Bayes (good for text and fast with high-dimensional data), but also try SVM for potentially higher accuracy. Naïve Bayes excels with text and imbalanced data; SVM needs more resources.\n",
        "\n",
        "Class Imbalance: Use resampling (e.g., SMOTE, oversample spam, or undersample non-spam), or adjust class weights ('class_weight' in SVM).\n",
        "\n",
        "Evaluation: Use metrics like Precision, Recall, F1-score, and ROC-AUC (not just accuracy) to judge performance—critical when legitimate emails outnumber spam.\n",
        "\n",
        "Business Value: Accurate spam detection saves user time, reduces risk of fraud/phishing, and improves overall email system reliability. Automated classification enables scalable monitoring and protects company reputation."
      ],
      "metadata": {
        "id": "L5f9TKUGdqsE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff91ed0e",
        "outputId": "e595a3c0-b9d3-409c-e41c-a2b24926138a"
      },
      "source": [
        "# Create a dummy emails.csv for demonstration\n",
        "data = {'text': ['This is a legitimate email.', 'Buy now and get free shipping!', 'Another important message.', 'Claim your prize today!'],\n",
        "        'label': ['not spam', 'spam', 'not spam', 'spam']}\n",
        "df_dummy = pd.DataFrame(data)\n",
        "df_dummy.to_csv('emails.csv', index=False)\n",
        "\n",
        "print(\"Dummy 'emails.csv' created.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dummy 'emails.csv' created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# 1. Data loading (assume a CSV with columns 'text', 'label')\n",
        "df = pd.read_csv('emails.csv')\n",
        "\n",
        "# 2. Preprocessing: Handle missing values\n",
        "df['text'] = df['text'].fillna('')\n",
        "df['label'] = df['label'].fillna('not spam')\n",
        "\n",
        "# 3. Encode labels: spam=1, not spam=0\n",
        "df['label_num'] = (df['label'] == 'spam').astype(int)\n",
        "\n",
        "# 4. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['text'], df['label_num'], test_size=0.2, random_state=42)\n",
        "\n",
        "# 5. Text vectorization using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=3000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# 6. Handle class imbalance using class weights\n",
        "class_weights = compute_class_weight('balanced', classes=np.array([0,1]), y=y_train)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# 7. Model training with SVM (linear kernel)\n",
        "clf = SVC(kernel='linear', class_weight=class_weight_dict, probability=True)\n",
        "clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# 8. Predictions\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "\n",
        "# 9. Evaluation\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=['Not Spam', 'Spam']))\n",
        "\n",
        "# Optional: Probability outputs, ROC curve, etc."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EsM_-5mfZSv",
        "outputId": "0a9e85dc-3523-4662-8c16-6b2626cadb57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            " [[0 0]\n",
            " [1 0]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    Not Spam       0.00      0.00      0.00       0.0\n",
            "        Spam       0.00      0.00      0.00       1.0\n",
            "\n",
            "    accuracy                           0.00       1.0\n",
            "   macro avg       0.00      0.00      0.00       1.0\n",
            "weighted avg       0.00      0.00      0.00       1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8yt6mrmNfmbe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}